{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys, os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import copy\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "from plotly.colors import n_colors\n",
    "import math\n",
    "import datetime as dt\n",
    "from sklearn.metrics import r2_score\n",
    "import plotly.colors\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import odr\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logging.basicConfig(format='%(asctime)s.%(msecs)03d %(message)s', datefmt='%Y-%m-%d %H:%M:%S')\n",
    "level = logging.getLevelName('INFO')\n",
    "logger.setLevel(level)\n",
    "\n",
    "# ----- Internal Dependencies -------#\n",
    "nb_dir = os.path.split(os.getcwd())[0]\n",
    "sys.path.append(\"../..\")\n",
    "if nb_dir not in sys.path:\n",
    "    sys.path.append(nb_dir)\n",
    "from General import FileRead\n",
    "from General import AccuracyMetrics\n",
    "from General.GeneralFunctions import get_add_to_dict\n",
    "from General import TableManipulations\n",
    "from FeatureGeneration import GeneralFeatures\n",
    "from General import FileWrite\n",
    "\n",
    "from Plot.PdfHelper import PdfHelper\n",
    "from Plot.PlotMaker import PlotMaker\n",
    "# from Plot import Voltage_Plots\n",
    "from Plot import PlotFunctions\n",
    "# from Plot import AdHocPlots\n",
    "# from Plot import Sim_Plots\n",
    "\n",
    "# widget and notebook stuff\n",
    "from ipywidgets import interact, fixed\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"plotly_mimetype+notebook\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "pathlib.WindowsPath = pathlib.PosixPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_linear_model(B, x):\n",
    "    return B[0]+B[1]*x\n",
    "\n",
    "def retro_cal_gluc_by_ref(biolinq_df,ref_df,col_to_match = 'UTC_Time',tol_to_match='7.5min',gluc_col_name='biolinq',ref_col_name='acck'):\n",
    "    biolinq_ref_df = pd.merge_asof(left=ref_df.dropna(subset=[col_to_match]).sort_values(by=col_to_match), right=biolinq_df.dropna(subset=[col_to_match]).sort_values(by=col_to_match),\n",
    "                                on=col_to_match, direction='nearest', tolerance=pd.Timedelta(tol_to_match))\n",
    "    biolinq_ref_df = biolinq_ref_df.dropna(subset=[ref_col_name])\n",
    "    biolinq_ref_df = biolinq_ref_df.dropna(subset=[gluc_col_name])\n",
    "    slope,intercept,rmse = fit_odr_gluc_ref(biolinq_ref_df,gluc_col_name,ref_col_name)\n",
    "    if (biolinq_ref_df.shape[0]>=10) and (biolinq_ref_df[ref_col_name].max()-biolinq_ref_df[ref_col_name].min()>=30) and (intercept>=-60) and (intercept<=60) and (slope>=0.4) and (slope<=1.6):\n",
    "        biolinq_df[f'{gluc_col_name}_adj']=biolinq_df[gluc_col_name].subtract(intercept).divide(slope)\n",
    "    else:\n",
    "        biolinq_df[f'{gluc_col_name}_adj']=biolinq_df[gluc_col_name]\n",
    "    return slope,intercept,rmse\n",
    "\n",
    "def fit_odr_gluc_ref(biolinq_ref_df,gluc_col_name,ref_col_name):\n",
    "    # Orthogonal regression\n",
    "    orthogonal_model = odr.Model(simple_linear_model)\n",
    "    current_data = odr.Data(biolinq_ref_df[ref_col_name],biolinq_ref_df[gluc_col_name])\n",
    "    orthogonal_reg = odr.ODR(current_data,orthogonal_model,beta0=[0.0,1.0])\n",
    "    orthogonal_reg_outcome = orthogonal_reg.run()\n",
    "    intercept = orthogonal_reg_outcome.beta[0]\n",
    "    slope = orthogonal_reg_outcome.beta[1]\n",
    "    rmse = np.sqrt(orthogonal_reg_outcome.res_var)\n",
    "    return slope,intercept,rmse\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load processed data\n",
    "# data_folder_parent = '/Users/liangwang/Library/CloudStorage/OneDrive-Biolinq/Documents - Clinical Data Analysis/Data/Aggregate/Isabella_v01'\n",
    "# data_dict={\n",
    "#     # 'eBlinq14':Path(data_folder_parent,'eBlinq14'),\n",
    "#     # 'eBlinq15':Path(data_folder_parent,'eBlinq15'),\n",
    "#     # 'eBlinq16':Path(data_folder_parent,'eBlinq16'),\n",
    "#     # 'eBlinq17':Path(data_folder_parent,'eBlinq17'),\n",
    "#     # 'eBlinq18':Path(data_folder_parent,'eBlinq18'),\n",
    "#     # 'eBlinq19a':Path(data_folder_parent,'eBlinq19a'),\n",
    "#     # 'eBlinq19b':Path(data_folder_parent,'eBlinq19b'),\n",
    "#     'eBlinq19c':Path(data_folder_parent,'eBlinq19c'),\n",
    "#     # 'eBlinq20':Path(data_folder_parent,'eBlinq20'),\n",
    "#     # 'eBlinq22':Path(data_folder_parent,'eBlinq22'),\n",
    "#     # 'eBlinq23':Path(data_folder_parent,'eBlinq23'),\n",
    "#     # 'eBlinq25':Path(data_folder_parent,'eBlinq25'),\n",
    "#     # 'eBlinqRingOverlay&NiMNA':Path(data_folder_parent,'eBlinqRingOverlay&NiMNA'),\n",
    "#     # 'iBlinq16':Path(data_folder_parent,'iBlinq16'),\n",
    "#     # 'iBlinqAcet':Path(data_folder_parent,'iBlinqAcet'),\n",
    "#     # 'iBlinqAdhesive':Path(data_folder_parent,'iBlinqAdhesive'),\n",
    "#     # 'iBlinqFW231':Path(data_folder_parent,'iBlinqFW231'),\n",
    "#     # 'iBlinqOverlay':Path(data_folder_parent,'iBlinqOverlay'),\n",
    "#     # 'Pre-Piv-2-training':Path(data_folder_parent,'Pre-Piv-2-training'),\n",
    "# }\n",
    "\n",
    "data_folder_parent = '/Volumes/Shared/Algo/Algorithm Output/new_run'\n",
    "data_dict={\n",
    "    'Model_May':Path(data_folder_parent,'eBlinqMayStudy')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Consolidated MFG data\n",
    "# mfg_data=pd.read_excel(r'C:\\Users\\elizabeth\\OneDrive - Biolinq Inc\\Data\\Aggregate\\eblinqiblinq 14-23\\training\\BLINQ TRENDING MONEY YIELD TABLE no formulas.xlsx')\n",
    "# mfg_data=pd.read_excel(r'/Users/liangwang/Library/CloudStorage/OneDrive-Biolinq/Documents - Clinical Data Analysis/Data/Aggregate/Algo Validation/Algo Validation - Sensor Map.xlsx',sheet_name='All')\n",
    "\n",
    "#List of features to gather from sample df\n",
    "cols_all=pd.read_excel('/Users/liangwang/Library/CloudStorage/OneDrive-Biolinq/Gen 1/Algorithm Development/Gen 1 Model Optimization/features_for_optimization.xlsx')['feature']\n",
    "\n",
    "#Columns to keep from sensor map\n",
    "cols_sensor_map=['Sensor_Id']\n",
    "\n",
    "#List of 300 Hz features\n",
    "cols_300=[\n",
    "'real1_300_alignment',\n",
    "'real3_300_alignment',\n",
    "'real4_300_alignment',\n",
    "'imag1_300_alignment',\n",
    "'imag3_300_alignment',\n",
    "'imag4_300_alignment',\n",
    "'deg1_300_featureGeneration',\n",
    "'mag1_300_featureGeneration',\n",
    "'deg3_300_featureGeneration',\n",
    "'mag3_300_featureGeneration',\n",
    "'deg4_300_featureGeneration',\n",
    "'mag4_300_featureGeneration',\n",
    "'alt_nyquistlength_cur1_featureGeneration',\n",
    "'alt_nyquistlength_cur3_featureGeneration',\n",
    "'alt_nyquistlength_cur4_featureGeneration',\n",
    "'alt_nyquistslope_cur1_featureGeneration',\n",
    "'alt_nyquistslope_cur3_featureGeneration',\n",
    "'alt_nyquistslope_cur4_featureGeneration',\n",
    "'diff_ewm_average_8hr_deg1_300_featureGeneration',\n",
    "'diff_ewm_average_8hr_deg3_300_featureGeneration',\n",
    "'diff_ewm_average_8hr_deg4_300_featureGeneration',\n",
    "'diff_ewm_average_8hr_mag1_300_featureGeneration',\n",
    "'diff_ewm_average_8hr_mag3_300_featureGeneration',\n",
    "'diff_ewm_average_8hr_mag4_300_featureGeneration',\n",
    "'estimate_standard_deviation_12hr_deg1_300_featureGeneration',\n",
    "'estimate_standard_deviation_12hr_deg3_300_featureGeneration',\n",
    "'estimate_standard_deviation_12hr_deg4_300_featureGeneration',\n",
    "'estimate_standard_deviation_12hr_mag1_300_featureGeneration',\n",
    "'estimate_standard_deviation_12hr_mag3_300_featureGeneration',\n",
    "'estimate_standard_deviation_12hr_mag4_300_featureGeneration',\n",
    "'estimate_standard_deviation_24hr_deg1_300_featureGeneration',\n",
    "'estimate_standard_deviation_24hr_deg3_300_featureGeneration',\n",
    "'estimate_standard_deviation_24hr_deg4_300_featureGeneration',\n",
    "'estimate_standard_deviation_24hr_mag1_300_featureGeneration',\n",
    "'estimate_standard_deviation_24hr_mag3_300_featureGeneration',\n",
    "'estimate_standard_deviation_24hr_mag4_300_featureGeneration',\n",
    "'ewm_average_8hr_deg1_300_featureGeneration',\n",
    "'ewm_average_8hr_deg3_300_featureGeneration',\n",
    "'ewm_average_8hr_deg4_300_featureGeneration',\n",
    "'ewm_average_8hr_mag1_300_featureGeneration',\n",
    "'ewm_average_8hr_mag3_300_featureGeneration',\n",
    "'ewm_average_8hr_mag4_300_featureGeneration',\n",
    "'ratio_mag1_10000_to_mag1_300_featureGeneration',\n",
    "'ratio_mag3_10000_to_mag3_300_featureGeneration',\n",
    "'ratio_mag4_10000_to_mag4_300_featureGeneration',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Whether to keep 300Hz features\n",
    "keep_300=False\n",
    "\n",
    "#Whether to merge mfg data\n",
    "keep_mfg=False\n",
    "\n",
    "#List of reference CSVs to create\n",
    "ref_to_use = 'cgm_adj'\n",
    "\n",
    "#Output folder\n",
    "file_path = '/Users/liangwang/Library/CloudStorage/OneDrive-Biolinq/Gen 1/Algorithm Development/Gen 1 Model Optimization'\n",
    "file_name = \"agg_sample_df_ref_May_Study_mfg.csv\"\n",
    "#Column to merge on\n",
    "time_to_match = 'UTC_Time'\n",
    "\n",
    "#Specify glucose channels\n",
    "ch_str = [\"1\",\"3\",\"4\"]\n",
    "gluc_prefix=\"model_output_s\"\n",
    "gluc_suffix=\"_calculateGlucose\"\n",
    "\n",
    "cgm_cols_needed = ['UTC_Time','cgm','cgm_adj','cgm_name']\n",
    "acck_cols_needed = ['UTC_Time','acck']\n",
    "ysi_cols_needed = ['UTC_Time','ysi']\n",
    "\n",
    "# rolling logics\n",
    "rolling_window = 2880 #24 hours\n",
    "rolling_min_size_calc = 100\n",
    "corr_tracking_thresh = 0.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#only keep specified columns\n",
    "if not keep_300:\n",
    "    cols_all=cols_all[~cols_all.isin(cols_300)].to_list()\n",
    "if False: # keep_mfg:\n",
    "    cols_all=pd.concat([cols_all,pd.Series(mfg_data.columns)]).to_list()\n",
    "\n",
    "cols_all=cols_all+cols_sensor_map\n",
    "cols_all.append(\"Study\")\n",
    "cols_all.append(\"Ref\")\n",
    "cols_all.append(\"RefType\")\n",
    "cols_all=list(set(cols_all))\n",
    "cols_all=sorted(cols_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RUN CONSOLIDATION ###\n",
    "\n",
    "#Output file name\n",
    "file_path_name=os.path.join(file_path,file_name)\n",
    "\n",
    "#check if data consolidation file already exists\n",
    "if os.path.isfile(file_path_name):\n",
    "    raise FileExistsError(\"File already exists. \"+file_path_name)\n",
    "\n",
    "#Save the header for the first sensor in the file\n",
    "head=False\n",
    "\n",
    "#load data study by study\n",
    "for i,ds in enumerate(data_dict.keys()):\n",
    "    logger.info(f'Processing {ds}')\n",
    "    #Load dataset\n",
    "    loaded_data=FileRead.load_pickle('alg_out.zip', data_dict[ds], as_dict=True)\n",
    "    sensor_map_df=loaded_data['sensor_map_df']\n",
    "\n",
    "    #Sensor for loop\n",
    "    for j,sensor in enumerate(loaded_data['all_sensor_data'].keys()):\n",
    "        logger.info(f'Aggregating {sensor}')\n",
    "        #Load data and create UTC Time column\n",
    "        current_sensor_data = loaded_data['all_sensor_data'][sensor]\n",
    "        sample_df_exist = False\n",
    "        if 'sample_df' in current_sensor_data.keys():\n",
    "            df = current_sensor_data['sample_df']\n",
    "            sample_df_exist = True\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        #Populate study column\n",
    "        df['Study']=ds\n",
    "\n",
    "        #Populate columns from sensor map\n",
    "        for c in cols_sensor_map:\n",
    "            df[c]=sensor_map_df[sensor_map_df['Sensor_Id']==sensor][c].values[0]\n",
    "\n",
    "        #Merge MFG data\n",
    "        if keep_mfg:\n",
    "            # df=pd.merge(left=df,right=mfg_data,how='left',on='SN')\n",
    "            df=pd.merge(left=df.drop(columns=['Study']),right=mfg_data.rename(columns={'Grouping':'Study'}),how='left',on='Sensor_Id')\n",
    "            \n",
    "        # pull all reference dataframe\n",
    "        acck_exist = False\n",
    "        ysi_exist = False\n",
    "        Libre_exist = False\n",
    "        Dexcom_exist = False\n",
    "        cgm_exist = False            \n",
    "        if 'acck' in current_sensor_data.keys():\n",
    "            acck_exist = True\n",
    "            acck_df = current_sensor_data['acck'].copy()\n",
    "        if 'ysi' in current_sensor_data.keys():\n",
    "            ysi_exist = True\n",
    "            ysi_df = current_sensor_data['ysi'].copy()\n",
    "        if 'Libre' in current_sensor_data.keys():\n",
    "            Libre_exist = True\n",
    "            Libre_df = current_sensor_data['Libre'].copy()\n",
    "        if 'Dexcom' in current_sensor_data.keys():\n",
    "            Dexcom_exist = True\n",
    "            Dexcom_df = current_sensor_data['Dexcom'].copy()            \n",
    "\n",
    "        #If reference is adjusted, merge data and perform orthogonal regression fit. \n",
    "        #Protect from too little data, too little glucose range, too large of slopes, and too large of intercepts\n",
    "        cgm_df = pd.DataFrame()\n",
    "        \n",
    "        if acck_exist and not acck_df.empty:\n",
    "            if Libre_exist and not Libre_df.empty:\n",
    "                _,_,_ = retro_cal_gluc_by_ref(Libre_df,acck_df,gluc_col_name='Libre')              \n",
    "                cgm_df = Libre_df\n",
    "                cgm_df.rename(columns={'Libre':'cgm'},inplace=True)\n",
    "                cgm_df.rename(columns={'Libre_adj': 'cgm_adj'},inplace=True)\n",
    "                cgm_df['cgm_name'] = 1\n",
    "                cgm_exist = True\n",
    "            elif Dexcom_exist and not Dexcom_df.empty:\n",
    "                _,_,_ = retro_cal_gluc_by_ref(Dexcom_df,acck_df,gluc_col_name='Dexcom')              \n",
    "                cgm_df = Dexcom_df\n",
    "                cgm_df.rename(columns={'Dexcom':'cgm'},inplace=True)\n",
    "                cgm_df.rename(columns={'Dexcom_adj': 'cgm_adj'},inplace=True)\n",
    "                cgm_df['cgm_name'] = 2       \n",
    "                cgm_exist = True\n",
    "\n",
    "        if  cgm_exist and not cgm_df.empty and all(e in cgm_df.columns for e in cgm_cols_needed):\n",
    "            df_cgm_paired = pd.merge_asof(left=df.dropna(subset=[time_to_match]).sort_values(by=time_to_match),right=cgm_df[cgm_cols_needed].dropna(subset=[time_to_match]).sort_values(by=time_to_match),on=time_to_match,direction='nearest',tolerance=pd.Timedelta('5min'))\n",
    "            for current_ch_str in ch_str:\n",
    "                current_col = gluc_prefix+current_ch_str+gluc_suffix\n",
    "                df_cgm_paired_roll_corr = df_cgm_paired[[current_col,'cgm_adj']].rolling(rolling_window,min_periods=rolling_min_size_calc,center=True).corr(pairwise=True)\n",
    "                df_cgm_paired_roll_corr.index.names = ['index','measurement']\n",
    "                df_cgm_paired[f'retro_roll_corr_s{current_ch_str}'] = df_cgm_paired_roll_corr.query(f'measurement==\"{current_col}\"').reset_index()['cgm_adj']\n",
    "                roll_ols_model= RollingOLS.from_formula(f'{current_col} ~ cgm_adj',data=df_cgm_paired,window=rolling_window,min_nobs=rolling_min_size_calc,missing='drop',expanding=True)\n",
    "                fit_result = roll_ols_model.fit()\n",
    "                df_cgm_paired[f'retro_roll_slope_s{current_ch_str}'] = fit_result.params['cgm_adj']\n",
    "                df_cgm_paired[f'retro_roll_intercept_s{current_ch_str}'] = fit_result.params['Intercept']\n",
    "                df_cgm_paired[f'retro_roll_rmse_s{current_ch_str}'] = fit_result.mse_resid.apply(np.sqrt)\n",
    "                df_cgm_paired[f'retro_roll_{current_col}'] = df_cgm_paired[current_col].sub(df_cgm_paired[f'retro_roll_intercept_s{current_ch_str}']).div(df_cgm_paired[f'retro_roll_slope_s{current_ch_str}'])\n",
    "                idx_tracking = df_cgm_paired[f'retro_roll_corr_s{current_ch_str}'].ge(corr_tracking_thresh)\n",
    "                df_cgm_paired_copy = df_cgm_paired.loc[idx_tracking].dropna(subset=[current_col,'cgm_adj']).copy()\n",
    "                slope,intercept,rmse = fit_odr_gluc_ref(df_cgm_paired_copy,current_col,'cgm_adj')                \n",
    "                df_cgm_paired[f'retro_slope_s{current_ch_str}'] = slope\n",
    "                df_cgm_paired[f'retro_intercept_s{current_ch_str}'] = intercept\n",
    "                df_cgm_paired[f'retro_rmse_s{current_ch_str}'] = rmse\n",
    "                df_cgm_paired[f'retro_{current_col}'] = df_cgm_paired[current_col].sub(df_cgm_paired[f'retro_intercept_s{current_ch_str}']).div(df_cgm_paired[f'retro_slope_s{current_ch_str}'])\n",
    "        else:\n",
    "            df_cgm_paired = df\n",
    "\n",
    "        if  acck_exist and not acck_df.empty and all(e in acck_df.columns for e in acck_cols_needed):\n",
    "            df_cgm_acck_paired = pd.merge_asof(left=df_cgm_paired.dropna(subset=[time_to_match]).sort_values(by=time_to_match),right=acck_df[acck_cols_needed].dropna(subset=[time_to_match]).sort_values(by=time_to_match),on=time_to_match,direction='nearest',tolerance=pd.Timedelta('5min'))\n",
    "        else:\n",
    "            df_cgm_acck_paired = df_cgm_paired\n",
    "\n",
    "        if  ysi_exist and not ysi_df.empty and all(e in ysi_df.columns for e in ysi_cols_needed):\n",
    "            df_cgm_acck_ysi_paired = pd.merge_asof(left=df_cgm_acck_paired.dropna(subset=[time_to_match]).sort_values(by=time_to_match),right=ysi_df[ysi_cols_needed].dropna(subset=[time_to_match]).sort_values(by=time_to_match),on=time_to_match,direction='nearest',tolerance=pd.Timedelta('5min'))\n",
    "        else:\n",
    "            df_cgm_acck_ysi_paired = df_cgm_acck_paired\n",
    "\n",
    "        #Clean paired data\n",
    "        for current_ch_str in ch_str:\n",
    "            current_col = gluc_prefix+current_ch_str+gluc_suffix\n",
    "            if current_col not in df_cgm_acck_ysi_paired.columns:\n",
    "                continue\n",
    "            df_cgm_acck_ysi_paired=df_cgm_acck_ysi_paired.dropna(subset=[current_col])                                        \n",
    "        if df_cgm_acck_ysi_paired.empty:\n",
    "            continue\n",
    "\n",
    "        #Fill in desired columns with NaN\n",
    "        for col in cols_all:\n",
    "            if col not in df_cgm_acck_ysi_paired.columns:\n",
    "                df_cgm_acck_ysi_paired[col]=np.nan\n",
    "        \n",
    "        #Save to CSV\n",
    "        if head==False:\n",
    "            df_cgm_acck_ysi_paired[cols_all].to_csv(file_path_name,mode='a',float_format='%g',index=False,header=True)\n",
    "            head=True\n",
    "        else:\n",
    "            df_cgm_acck_ysi_paired[cols_all].to_csv(file_path_name,mode='a',float_format='%g',index=False,header=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".datarunner",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
